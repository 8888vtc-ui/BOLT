# Intervention Serveur Ollama sur Railway

**URL du Serveur**: `https://bot-production-b9d6.up.railway.app`  
**Probl√®me**: Erreur 500 sur `/api/generate` et `/api/chat`

---

## üéØ Actions Imm√©diates √† Effectuer

### 1. Acc√©der √† Railway Dashboard

1. Aller sur https://railway.app
2. Se connecter avec votre compte
3. Trouver le projet contenant le service Ollama
4. Ouvrir le service Ollama

---

### 2. Consulter les Logs (PRIORIT√â 1)

**Dans Railway Dashboard**:
1. Cliquer sur le service Ollama
2. Aller dans l'onglet **"Logs"** ou **"Deployments"**
3. Consulter les logs r√©cents
4. Chercher les erreurs autour de l'heure actuelle

**Ce qu'il faut chercher**:
- `Error 500`
- `Out of memory` ou `OOM`
- `Model not found`
- `Failed to load model`
- `Timeout`

**Action**: Copier les messages d'erreur pour diagnostic

---

### 3. Red√©marrer le Serveur (Solution Rapide)

**Dans Railway Dashboard**:
1. Ouvrir le service Ollama
2. Cliquer sur **"Settings"** ou **"..."** (menu)
3. S√©lectionner **"Restart"** ou **"Redeploy"**
4. Attendre 1-2 minutes
5. Tester l'API √† nouveau

**Via Railway CLI** (si install√©):
```bash
railway login
railway link  # Lier au projet
railway restart
```

---

### 4. V√©rifier les Variables d'Environnement

**Dans Railway Dashboard**:
1. Ouvrir le service Ollama
2. Aller dans **"Variables"**
3. V√©rifier:
   - `OLLAMA_MODEL=deepseek-coder:latest` ‚úÖ
   - `OLLAMA_HOST` (si pr√©sent)
   - `OLLAMA_PORT` (si pr√©sent)
   - Variables de m√©moire/ressources

**Si manquant**: Ajouter `OLLAMA_MODEL=deepseek-coder:latest`

---

### 5. V√©rifier les Ressources

**Dans Railway Dashboard**:
1. Ouvrir le service Ollama
2. Aller dans **"Settings"** ‚Üí **"Resources"**
3. V√©rifier:
   - **RAM disponible**: Le mod√®le `deepseek-coder` n√©cessite ~2-4GB
   - **Plan Railway**: Plan gratuit = 512MB (insuffisant pour ce mod√®le)

**Si RAM insuffisante**:
- Option 1: Passer √† un plan payant Railway (recommand√©)
- Option 2: Utiliser un mod√®le plus l√©ger (voir ci-dessous)

---

### 6. Tester Apr√®s Red√©marrage

**Test rapide avec PowerShell**:

```powershell
# Test /api/generate
$body = @{
    model = "deepseek-coder:latest"
    prompt = "Hello"
    stream = $false
} | ConvertTo-Json

try {
    $response = Invoke-WebRequest -Uri "https://bot-production-b9d6.up.railway.app/api/generate" `
        -Method POST -Body $body -ContentType "application/json" -TimeoutSec 30
    Write-Host "‚úÖ Succ√®s! Status: $($response.StatusCode)"
    $data = $response.Content | ConvertFrom-Json
    Write-Host "R√©ponse: $($data.response)"
} catch {
    Write-Host "‚ùå Erreur: $($_.Exception.Message)"
}
```

---

## üîß Solutions Selon le Probl√®me

### Probl√®me: M√©moire Insuffisante (OOM)

**Sympt√¥mes dans les logs**:
- `Out of memory`
- `OOM`
- `Killed`

**Solutions**:
1. **Augmenter les ressources Railway** (plan payant)
2. **Utiliser un mod√®le plus l√©ger**:
   - `mistral:7b` (~4GB)
   - `llama2:7b` (~4GB)
   - `phi:2.7b` (~2GB)
3. **Modifier le code** pour utiliser un mod√®le plus l√©ger

---

### Probl√®me: Mod√®le Non Charg√©

**Sympt√¥mes dans les logs**:
- `Model not found`
- `Failed to load model`

**Solutions**:
1. **V√©rifier que le mod√®le est t√©l√©charg√©**:
   ```bash
   # Via Railway CLI ou terminal du service
   ollama list
   ```
2. **T√©l√©charger le mod√®le**:
   ```bash
   ollama pull deepseek-coder:latest
   ```
3. **V√©rifier la variable d'environnement** `OLLAMA_MODEL`

---

### Probl√®me: Timeout

**Sympt√¥mes**:
- Requ√™tes qui prennent > 30 secondes
- Erreur timeout

**Solutions**:
1. **Augmenter le timeout** dans le code (d√©j√† √† 30s)
2. **R√©duire `num_predict`** dans les options Ollama
3. **Utiliser un mod√®le plus rapide**

---

### Probl√®me: Configuration Incorrecte

**Sympt√¥mes**:
- Erreurs de configuration dans les logs
- Variables d'environnement manquantes

**Solutions**:
1. V√©rifier toutes les variables d'environnement
2. V√©rifier le Dockerfile/configuration Railway
3. Red√©ployer le service

---

## üöÄ Solution Alternative: DeepSeek API (Recommand√©)

Si le serveur Ollama continue √† poser probl√®me, utilisez le fallback DeepSeek API :

### √âtapes

1. **Obtenir une Cl√© API DeepSeek**
   - Aller sur https://platform.deepseek.com
   - Cr√©er un compte (gratuit)
   - Aller dans "API Keys"
   - Cr√©er une nouvelle cl√©

2. **Configurer dans le Projet Local**
   - Cr√©er/modifier `.env` √† la racine de `D:\BOLT\BOLT`
   - Ajouter:
     ```
     VITE_DEEPSEEK_API_KEY=sk-votre_cle_api_ici
     ```
   - Red√©marrer le serveur de d√©veloppement (`npm run dev`)

3. **Test**
   - Le coach utilisera automatiquement DeepSeek API si Ollama √©choue
   - Fonctionne imm√©diatement
   - Co√ªt: ~$0.14 pour 1M tokens (tr√®s √©conomique)

---

## üìã Checklist Compl√®te

### Diagnostic
- [ ] Consulter les logs Railway
- [ ] Identifier le type d'erreur (OOM, timeout, mod√®le, etc.)
- [ ] V√©rifier les variables d'environnement
- [ ] V√©rifier les ressources (RAM, CPU)

### Actions Correctives
- [ ] Red√©marrer le serveur Ollama
- [ ] V√©rifier que le mod√®le est charg√©
- [ ] Augmenter les ressources si n√©cessaire
- [ ] Tester l'API apr√®s corrections

### Alternative
- [ ] Si √©chec ‚Üí Configurer DeepSeek API fallback
- [ ] Tester le coach avec DeepSeek API
- [ ] Documenter la solution

---

## üÜò Commandes Utiles Railway

### Via Railway CLI

```bash
# Installer Railway CLI
npm i -g @railway/cli

# Se connecter
railway login

# Lier au projet
railway link

# Voir les logs en temps r√©el
railway logs

# Red√©marrer le service
railway restart

# Voir les variables d'environnement
railway variables

# Ouvrir un shell dans le service
railway shell
```

### Via Railway Dashboard

1. **Logs**: Service ‚Üí Onglet "Logs"
2. **Variables**: Service ‚Üí Onglet "Variables"
3. **Settings**: Service ‚Üí Onglet "Settings"
4. **Restart**: Service ‚Üí Menu "..." ‚Üí "Restart"

---

## ‚úÖ R√©sultat Attendu

Apr√®s intervention r√©ussie:
- ‚úÖ `/api/generate` retourne Status 200
- ‚úÖ `/api/chat` retourne Status 200
- ‚úÖ Le coach r√©pond aux questions
- ‚úÖ Les r√©ponses sont en fran√ßais

---

## üìû Support

Si le probl√®me persiste:
1. Consulter la documentation Railway: https://docs.railway.app
2. Consulter la documentation Ollama: https://ollama.ai/docs
3. V√©rifier les issues GitHub Railway/Ollama
4. Utiliser le fallback DeepSeek API en attendant


